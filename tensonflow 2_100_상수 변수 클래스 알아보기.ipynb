{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0-dev20191015'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OVERLOADABLE_OPERATORS',\n",
       " '_USE_EQUALITY',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__copy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__invert__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rfloordiv__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_as_node_def_input',\n",
       " '_as_tf_output',\n",
       " '_c_api_shape',\n",
       " '_create_with_tf_output',\n",
       " '_disallow_bool_casting',\n",
       " '_disallow_in_graph_mode',\n",
       " '_disallow_iteration',\n",
       " '_disallow_when_autograph_disabled',\n",
       " '_disallow_when_autograph_enabled',\n",
       " '_maybe_constant_shape',\n",
       " '_override_operator',\n",
       " '_rank',\n",
       " '_shape',\n",
       " '_shape_as_list',\n",
       " '_shape_tuple',\n",
       " '_tf_api_names',\n",
       " '_tf_api_names_v1',\n",
       " 'consumers',\n",
       " 'device',\n",
       " 'dtype',\n",
       " 'eval',\n",
       " 'experimental_ref',\n",
       " 'get_shape',\n",
       " 'graph',\n",
       " 'name',\n",
       " 'op',\n",
       " 'set_shape',\n",
       " 'shape',\n",
       " 'value_index']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__closure__',\n",
       " '__code__',\n",
       " '__defaults__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get__',\n",
       " '__getattribute__',\n",
       " '__globals__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__kwdefaults__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__name__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__qualname__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_tf_api_names',\n",
       " '_tf_api_names_v1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상수도 실제 생성되면 tensor로 만들어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=100>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function constant in module tensorflow.python.framework.constant_op:\n",
      "\n",
      "constant(value, dtype=None, shape=None, name='Const')\n",
      "    Creates a constant tensor from a tensor-like object.\n",
      "    \n",
      "    Note: All eager `tf.Tensor` values are immutable (in contrast to\n",
      "    `tf.Variable`). There is nothing especially _constant_ about the value\n",
      "    returned from `tf.constant`. This function it is not fundamentally different\n",
      "    from `tf.convert_to_tensor`. The name `tf.constant` comes from the symbolic\n",
      "    APIs (like `tf.data` or keras functional models) where the `value` is embeded\n",
      "    in a `Const` node in the `tf.Graph`. `tf.constant` is useful for asserting\n",
      "    that the value can be embedded that way.\n",
      "    \n",
      "    If the argument `dtype` is not specified, then the type is inferred from\n",
      "    the type of `value`.\n",
      "    \n",
      "    >>> # Constant 1-D Tensor from a python list.\n",
      "    >>> tf.constant([1, 2, 3, 4, 5, 6])\n",
      "    <tf.Tensor: shape=(6,), dtype=int32,\n",
      "        numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>\n",
      "    >>> # Or a numpy array\n",
      "    >>> a = np.array([[1, 2, 3], [4, 5, 6]])\n",
      "    >>> tf.constant(a)\n",
      "    <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n",
      "      array([[1, 2, 3],\n",
      "             [4, 5, 6]])>\n",
      "    \n",
      "    If `dtype` is specified the resulting tensor values are cast to the requested\n",
      "    `dtype`.\n",
      "    \n",
      "    >>> tf.constant([1, 2, 3, 4, 5, 6], dtype=tf.float64)\n",
      "    <tf.Tensor: shape=(6,), dtype=float64,\n",
      "        numpy=array([1., 2., 3., 4., 5., 6.])>\n",
      "    \n",
      "    If `shape` is set, the `value` is reshaped to match. Scalars are expanded to\n",
      "    fill the `shape`:\n",
      "    \n",
      "    >>> tf.constant(0, shape=(2, 3))\n",
      "      <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
      "      array([[0, 0, 0],\n",
      "             [0, 0, 0]], dtype=int32)>\n",
      "    >>> tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      "    <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
      "      array([[1, 2, 3],\n",
      "             [4, 5, 6]], dtype=int32)>\n",
      "    \n",
      "    `tf.constant` has no effect if an eager Tensor is passed as the `value`, it\n",
      "    even transmits gradients:\n",
      "    \n",
      "    >>> v = tf.Variable([0.0])\n",
      "    >>> with tf.GradientTape() as g:\n",
      "    ...     loss = tf.constant(v + v)\n",
      "    >>> g.gradient(loss, v).numpy()\n",
      "    array([2.], dtype=float32)\n",
      "    \n",
      "    But, since `tf.constant` embeds the value in the `tf.Graph` this fails for\n",
      "    symbolic tensors:\n",
      "    \n",
      "    >>> i = tf.keras.layers.Input(shape=[None, None])\n",
      "    >>> t = tf.constant(i)\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: ...\n",
      "    \n",
      "    Related Ops:\n",
      "    \n",
      "    * `tf.convert_to_tensor` is similar but:\n",
      "      * It has no `shape` argument.\n",
      "      * Symbolic tensors are allowed to pass through.\n",
      "    \n",
      "        >>> i = tf.keras.layers.Input(shape=[None, None])\n",
      "        >>> t = tf.convert_to_tensor(i)\n",
      "    \n",
      "    * `tf.fill`: differs in a few ways:\n",
      "      *   `tf.constant` supports arbitrary constants, not just uniform scalar\n",
      "          Tensors like `tf.fill`.\n",
      "      *   `tf.fill` creates an Op in the graph that is expanded at runtime, so it\n",
      "          can efficiently represent large tensors.\n",
      "      *   Since `tf.fill` does not embed the value, it can produce dynamically\n",
      "          sized outputs.\n",
      "    \n",
      "    Args:\n",
      "      value: A constant value (or list) of output type `dtype`.\n",
      "      dtype: The type of the elements of the resulting tensor.\n",
      "      shape: Optional dimensions of resulting tensor.\n",
      "      name: Optional name for the tensor.\n",
      "    \n",
      "    Returns:\n",
      "      A Constant Tensor.\n",
      "    \n",
      "    Raises:\n",
      "      TypeError: if shape is incorrectly specified or unsupported.\n",
      "      ValueError: if called on a symbolic tensor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 변수 클래스 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SaveSliceInfo',\n",
       " '_OverloadAllOperators',\n",
       " '_OverloadOperator',\n",
       " '_TensorConversionFunction',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__array_priority__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__invert__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rfloordiv__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_add_variable_with_custom_getter',\n",
       " '_checkpoint_dependencies',\n",
       " '_deferred_dependencies',\n",
       " '_gather_saveables_for_checkpoint',\n",
       " '_get_save_slice_info',\n",
       " '_handle_deferred_dependencies',\n",
       " '_list_extra_dependencies_for_serialization',\n",
       " '_list_functions_for_serialization',\n",
       " '_lookup_dependency',\n",
       " '_maybe_initialize_trackable',\n",
       " '_name_based_attribute_restore',\n",
       " '_name_based_restores',\n",
       " '_no_dependency',\n",
       " '_object_identifier',\n",
       " '_preload_simple_restoration',\n",
       " '_restore_from_checkpoint_position',\n",
       " '_set_save_slice_info',\n",
       " '_setattr_tracking',\n",
       " '_shared_name',\n",
       " '_single_restoration_from_checkpoint_position',\n",
       " '_tf_api_names',\n",
       " '_tf_api_names_v1',\n",
       " '_track_trackable',\n",
       " '_tracking_metadata',\n",
       " '_unconditional_checkpoint_dependencies',\n",
       " '_unconditional_dependency_names',\n",
       " '_update_uid',\n",
       " 'aggregation',\n",
       " 'assign',\n",
       " 'assign_add',\n",
       " 'assign_sub',\n",
       " 'batch_scatter_update',\n",
       " 'constraint',\n",
       " 'count_up_to',\n",
       " 'device',\n",
       " 'dtype',\n",
       " 'eval',\n",
       " 'experimental_ref',\n",
       " 'from_proto',\n",
       " 'gather_nd',\n",
       " 'get_shape',\n",
       " 'graph',\n",
       " 'initial_value',\n",
       " 'initialized_value',\n",
       " 'initializer',\n",
       " 'load',\n",
       " 'name',\n",
       " 'op',\n",
       " 'read_value',\n",
       " 'scatter_add',\n",
       " 'scatter_div',\n",
       " 'scatter_max',\n",
       " 'scatter_min',\n",
       " 'scatter_mul',\n",
       " 'scatter_nd_add',\n",
       " 'scatter_nd_sub',\n",
       " 'scatter_nd_update',\n",
       " 'scatter_sub',\n",
       " 'scatter_update',\n",
       " 'set_shape',\n",
       " 'shape',\n",
       " 'sparse_read',\n",
       " 'synchronization',\n",
       " 'to_proto',\n",
       " 'trainable',\n",
       " 'value']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Variable in module tensorflow.python.ops.variables:\n",
      "\n",
      "class Variable(tensorflow.python.training.tracking.base.Trackable)\n",
      " |  Variable(*args, **kwargs)\n",
      " |  \n",
      " |  See the [variable guide](https://tensorflow.org/guide/variable).\n",
      " |  \n",
      " |  A variable maintains shared, persistent state manipulated by a program.\n",
      " |  \n",
      " |  The `Variable()` constructor requires an initial value for the variable, which\n",
      " |  can be a `Tensor` of any type and shape. This initial value defines the type\n",
      " |  and shape of the variable. After construction, the type and shape of the\n",
      " |  variable are fixed. The value can be changed using one of the assign methods.\n",
      " |  \n",
      " |  >>> v = tf.Variable(1.)\n",
      " |  >>> v.assign(2.)\n",
      " |  <tf.Variable ... shape=() dtype=float32, numpy=2.0>\n",
      " |  >>> v.assign_add(0.5)\n",
      " |  <tf.Variable ... shape=() dtype=float32, numpy=2.5>\n",
      " |  \n",
      " |  The `shape` argument to `Variable`'s constructor allows you to construct a\n",
      " |  variable with a less defined shape than its `initial_value`:\n",
      " |  \n",
      " |  >>> v = tf.Variable(1., shape=tf.TensorShape(None))\n",
      " |  >>> v.assign([[1.]])\n",
      " |  <tf.Variable ... shape=<unknown> dtype=float32, numpy=array([[1.]], ...)>\n",
      " |  \n",
      " |  Just like any `Tensor`, variables created with `Variable()` can be used as\n",
      " |  inputs to operations. Additionally, all the operators overloaded for the\n",
      " |  `Tensor` class are carried over to variables.\n",
      " |  \n",
      " |  >>> w = tf.Variable([[1.], [2.]])\n",
      " |  >>> x = tf.constant([[3., 4.]])\n",
      " |  >>> tf.matmul(w, x)\n",
      " |  <tf.Tensor:... shape=(2, 2), ... numpy=\n",
      " |    array([[3., 4.],\n",
      " |           [6., 8.]], dtype=float32)>\n",
      " |  >>> tf.sigmoid(w + x)\n",
      " |  <tf.Tensor:... shape=(2, 2), ...>\n",
      " |  \n",
      " |  When building a machine learning model it is often convenient to distinguish\n",
      " |  between variables holding trainable model parameters and other variables such\n",
      " |  as a `step` variable used to count training steps. To make this easier, the\n",
      " |  variable constructor supports a `trainable=<bool>`\n",
      " |  parameter. `tf.GradientTape` watches trainable variables by default:\n",
      " |  \n",
      " |  >>> with tf.GradientTape(persistent=True) as tape:\n",
      " |  ...   trainable = tf.Variable(1.)\n",
      " |  ...   non_trainable = tf.Variable(2., trainable=False)\n",
      " |  ...   x1 = trainable * 2.\n",
      " |  ...   x2 = non_trainable * 3.\n",
      " |  >>> tape.gradient(x1, trainable)\n",
      " |  <tf.Tensor:... shape=(), dtype=float32, numpy=2.0>\n",
      " |  >>> assert tape.gradient(x2, non_trainable) is None  # Unwatched\n",
      " |  \n",
      " |  Variables are automatically tracked when assigned to attributes of types\n",
      " |  inheriting from `tf.Module`.\n",
      " |  \n",
      " |  >>> m = tf.Module()\n",
      " |  >>> m.v = tf.Variable([1.])\n",
      " |  >>> m.trainable_variables\n",
      " |  (<tf.Variable ... shape=(1,) ... numpy=array([1.], dtype=float32)>,)\n",
      " |  \n",
      " |  This tracking then allows saving variable values to\n",
      " |  [training checkpoints](https://www.tensorflow.org/guide/checkpoint), or to\n",
      " |  [SavedModels](https://www.tensorflow.org/guide/saved_model) which include\n",
      " |  serialized TensorFlow graphs.\n",
      " |  \n",
      " |  Variables are often captured and manipulated by `tf.function`s. This works the\n",
      " |  same way the un-decorated function would have:\n",
      " |  \n",
      " |  >>> v = tf.Variable(0.)\n",
      " |  >>> read_and_decrement = tf.function(lambda: v.assign_sub(0.1))\n",
      " |  >>> read_and_decrement()\n",
      " |  <tf.Tensor: shape=(), dtype=float32, numpy=-0.1>\n",
      " |  >>> read_and_decrement()\n",
      " |  <tf.Tensor: shape=(), dtype=float32, numpy=-0.2>\n",
      " |  \n",
      " |  Variables created inside a `tf.function` must be owned outside the function\n",
      " |  and be created only once:\n",
      " |  \n",
      " |  >>> class M(tf.Module):\n",
      " |  ...   @tf.function\n",
      " |  ...   def __call__(self, x):\n",
      " |  ...     if not hasattr(self, \"v\"):  # Or set self.v to None in __init__\n",
      " |  ...       self.v = tf.Variable(x)\n",
      " |  ...     return self.v * x\n",
      " |  >>> m = M()\n",
      " |  >>> m(2.)\n",
      " |  <tf.Tensor: shape=(), dtype=float32, numpy=4.0>\n",
      " |  >>> m(3.)\n",
      " |  <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n",
      " |  >>> m.v\n",
      " |  <tf.Variable ... shape=() dtype=float32, numpy=2.0>\n",
      " |  \n",
      " |  See the `tf.function` documentation for details.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Variable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __abs__ = abs(x, name=None)\n",
      " |      Computes the absolute value of a tensor.\n",
      " |      \n",
      " |      Given a tensor of integer or floating-point values, this operation returns a\n",
      " |      tensor of the same type, where each element contains the absolute value of the\n",
      " |      corresponding element in the input.\n",
      " |      \n",
      " |      Given a tensor `x` of complex numbers, this operation returns a tensor of type\n",
      " |      `float32` or `float64` that is the absolute value of each element in `x`. All\n",
      " |      elements in `x` must be complex numbers of the form \\\\(a + bj\\\\). The\n",
      " |      absolute value is computed as \\\\( \\sqrt{a^2 + b^2}\\\\).  For example:\n",
      " |      ```python\n",
      " |      x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])\n",
      " |      tf.abs(x)  # [5.25594902, 6.60492229]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,\n",
      " |          `int32`, `int64`, `complex64` or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` or `SparseTensor` the same size, type, and sparsity as `x` with\n",
      " |          absolute values.\n",
      " |        Note, for `complex64` or `complex128` input, the returned `Tensor` will be\n",
      " |          of type `float32` or `float64`, respectively.\n",
      " |  \n",
      " |  __add__ = binary_op_wrapper(x, y)\n",
      " |      Dispatches to add for strings and add_v2 for all other types.\n",
      " |  \n",
      " |  __and__ = binary_op_wrapper(x, y)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_and` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __div__ = binary_op_wrapper(x, y)\n",
      " |      Divide two values using Python 2 semantics.\n",
      " |      \n",
      " |      Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Compares two variables element-wise for equality.\n",
      " |  \n",
      " |  __floordiv__ = binary_op_wrapper(x, y)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.compat.v1.div(x,y)` for integers, but uses\n",
      " |      `tf.floor(tf.compat.v1.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __ge__ = greater_equal(x, y, name=None)\n",
      " |      Returns the truth value of (x >= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.greater_equal` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([5, 4, 6, 7])\n",
      " |      y = tf.constant([5, 2, 5, 10])\n",
      " |      tf.math.greater_equal(x, y) ==> [True, True, True, False]\n",
      " |      \n",
      " |      x = tf.constant([5, 4, 6, 7])\n",
      " |      y = tf.constant([5])\n",
      " |      tf.math.greater_equal(x, y) ==> [True, False, True, True]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __getitem__ = _SliceHelperVar(var, slice_spec)\n",
      " |      Creates a slice helper object given a variable.\n",
      " |      \n",
      " |      This allows creating a sub-tensor from part of the current contents\n",
      " |      of a variable. See `tf.Tensor.__getitem__` for detailed examples\n",
      " |      of slicing.\n",
      " |      \n",
      " |      This function in addition also allows assignment to a sliced range.\n",
      " |      This is similar to `__setitem__` functionality in Python. However,\n",
      " |      the syntax is different so that the user can capture the assignment\n",
      " |      operation for grouping or passing to `sess.run()`.\n",
      " |      For example,\n",
      " |      \n",
      " |      ```python\n",
      " |      import tensorflow as tf\n",
      " |      A = tf.Variable([[1,2,3], [4,5,6], [7,8,9]], dtype=tf.float32)\n",
      " |      with tf.compat.v1.Session() as sess:\n",
      " |        sess.run(tf.compat.v1.global_variables_initializer())\n",
      " |        print(sess.run(A[:2, :2]))  # => [[1,2], [4,5]]\n",
      " |      \n",
      " |        op = A[:2,:2].assign(22. * tf.ones((2, 2)))\n",
      " |        print(sess.run(op))  # => [[22, 22, 3], [22, 22, 6], [7,8,9]]\n",
      " |      ```\n",
      " |      \n",
      " |      Note that assignments currently do not support NumPy broadcasting\n",
      " |      semantics.\n",
      " |      \n",
      " |      Args:\n",
      " |        var: An `ops.Variable` object.\n",
      " |        slice_spec: The arguments to `Tensor.__getitem__`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The appropriate slice of \"tensor\", based on \"slice_spec\".\n",
      " |        As an operator. The operator also has a `assign()` method\n",
      " |        that can be used to generate an assignment operator.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If a slice range is negative size.\n",
      " |        TypeError: TypeError: If the slice indices aren't int, slice,\n",
      " |          ellipsis, tf.newaxis or int32/int64 tensors.\n",
      " |  \n",
      " |  __gt__ = greater(x, y, name=None)\n",
      " |      Returns the truth value of (x > y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.greater` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5, 2, 5])\n",
      " |      tf.math.greater(x, y) ==> [False, True, True]\n",
      " |      \n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5])\n",
      " |      tf.math.greater(x, y) ==> [False, False, True]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __init__(self, initial_value=None, trainable=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, import_scope=None, constraint=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, shape=None)\n",
      " |      Creates a new variable with value `initial_value`. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(caching_device)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      A variable's value can be manually cached by calling tf.Variable.read_value() under a tf.device scope. The caching_device argument does not work properly.\n",
      " |      \n",
      " |      Args:\n",
      " |        initial_value: A `Tensor`, or Python object convertible to a `Tensor`,\n",
      " |          which is the initial value for the Variable. The initial value must have\n",
      " |          a shape specified unless `validate_shape` is set to False. Can also be a\n",
      " |          callable with no argument that returns the initial value when called. In\n",
      " |          that case, `dtype` must be specified. (Note that initializer functions\n",
      " |          from init_ops.py must first be bound to a shape before being used here.)\n",
      " |        trainable: If `True`, GradientTapes automatically watch uses of this\n",
      " |          variable. Defaults to `True`, unless `synchronization` is set to\n",
      " |          `ON_READ`, in which case it defaults to `False`.\n",
      " |        validate_shape: If `False`, allows the variable to be initialized with a\n",
      " |          value of unknown shape. If `True`, the default, the shape of\n",
      " |          `initial_value` must be known.\n",
      " |        caching_device: Optional device string describing where the Variable\n",
      " |          should be cached for reading.  Defaults to the Variable's device. If not\n",
      " |          `None`, caches on another device.  Typical use is to cache on the device\n",
      " |          where the Ops using the Variable reside, to deduplicate copying through\n",
      " |          `Switch` and other conditional statements.\n",
      " |        name: Optional name for the variable. Defaults to `'Variable'` and gets\n",
      " |          uniquified automatically.\n",
      " |        variable_def: `VariableDef` protocol buffer. If not `None`, recreates the\n",
      " |          Variable object with its contents, referencing the variable's nodes in\n",
      " |          the graph, which must already exist. The graph is not changed.\n",
      " |          `variable_def` and the other arguments are mutually exclusive.\n",
      " |        dtype: If set, initial_value will be converted to the given type. If\n",
      " |          `None`, either the datatype will be kept (if `initial_value` is a\n",
      " |          Tensor), or `convert_to_tensor` will decide.\n",
      " |        import_scope: Optional `string`. Name scope to add to the `Variable.` Only\n",
      " |          used when initializing from protocol buffer.\n",
      " |        constraint: An optional projection function to be applied to the variable\n",
      " |          after being updated by an `Optimizer` (e.g. used to implement norm\n",
      " |          constraints or value constraints for layer weights). The function must\n",
      " |          take as input the unprojected Tensor representing the value of the\n",
      " |          variable and return the Tensor for the projected value (which must have\n",
      " |          the same shape). Constraints are not safe to use when doing asynchronous\n",
      " |          distributed training.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses when to\n",
      " |          synchronize.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        shape: (optional) The shape of this variable. If None, the shape of\n",
      " |          `initial_value` will be used. When setting this argument to\n",
      " |          `tf.TensorShape(None)` (representing an unspecified shape), the variable\n",
      " |          can be assigned with values of different shapes.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If both `variable_def` and initial_value are specified.\n",
      " |        ValueError: If the initial value is not specified, or does not have a\n",
      " |          shape and `validate_shape` is `True`.\n",
      " |  \n",
      " |  __invert__ = logical_not(x, name=None)\n",
      " |      Returns the truth value of NOT x element-wise.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Dummy method to prevent iteration.\n",
      " |      \n",
      " |      Do not call.\n",
      " |      \n",
      " |      NOTE(mrry): If we register __getitem__ as an overloaded operator,\n",
      " |      Python will valiantly attempt to iterate over the variable's Tensor from 0\n",
      " |      to infinity.  Declaring this method prevents this unintended behavior.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: when invoked.\n",
      " |  \n",
      " |  __le__ = less_equal(x, y, name=None)\n",
      " |      Returns the truth value of (x <= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.less_equal` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5])\n",
      " |      tf.math.less_equal(x, y) ==> [True, True, False]\n",
      " |      \n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5, 6, 6])\n",
      " |      tf.math.less_equal(x, y) ==> [True, True, True]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __lt__ = less(x, y, name=None)\n",
      " |      Returns the truth value of (x < y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.less` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5])\n",
      " |      tf.math.less(x, y) ==> [False, True, False]\n",
      " |      \n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5, 6, 7])\n",
      " |      tf.math.less(x, y) ==> [False, True, True]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __matmul__ = binary_op_wrapper(x, y)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication dimensions,\n",
      " |      and any further outer dimensions specify matching batch size.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      A simple 2-D tensor matrix multiplication:\n",
      " |      >>> a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      >>> a  # 2-D tensor\n",
      " |      <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
      " |      array([[1, 2, 3],\n",
      " |             [4, 5, 6]], dtype=int32)>\n",
      " |      >>> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      >>> b  # 2-D tensor\n",
      " |      <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      " |      array([[ 7,  8],\n",
      " |             [ 9, 10],\n",
      " |             [11, 12]], dtype=int32)>\n",
      " |      >>> c = tf.matmul(a, b)\n",
      " |      >>> c  # `a` * `b`\n",
      " |      <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
      " |      array([[ 58,  64],\n",
      " |             [139, 154]], dtype=int32)>\n",
      " |      \n",
      " |      A batch matrix multiplication with batch shape [2]\n",
      " |      >>> a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])\n",
      " |      >>> a  # 3-D tensor\n",
      " |      <tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
      " |      array([[[ 1,  2,  3],\n",
      " |              [ 4,  5,  6]],\n",
      " |             [[ 7,  8,  9],\n",
      " |              [10, 11, 12]]], dtype=int32)>\n",
      " |      >>> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])\n",
      " |      >>> b  # 3-D tensor\n",
      " |      <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
      " |      array([[[13, 14],\n",
      " |              [15, 16],\n",
      " |              [17, 18]],\n",
      " |             [[19, 20],\n",
      " |              [21, 22],\n",
      " |              [23, 24]]], dtype=int32)>\n",
      " |      >>> c = tf.matmul(a, b)\n",
      " |      >>> c  # `a` * `b`\n",
      " |      <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
      " |      array([[[ 94, 100],\n",
      " |              [229, 244]],\n",
      " |             [[508, 532],\n",
      " |              [697, 730]]], dtype=int32)>\n",
      " |      \n",
      " |      Since python >= 3.5 the @ operator is supported\n",
      " |      (see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow,\n",
      " |      it simply calls the `tf.matmul()` function, so the following lines are\n",
      " |      equivalent:\n",
      " |      >>> d = a @ b @ [[10], [11]]\n",
      " |      >>> d = tf.matmul(tf.matmul(a, b), [[10], [11]])\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `tf.Tensor` of type `float16`, `float32`, `float64`, `int32`,\n",
      " |          `complex64`, `complex128` and rank > 1.\n",
      " |        b: `tf.Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of the same type as `a` and `b` where each inner-most matrix\n",
      " |        is the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])`,\n",
      " |        for all indices `i`, `j`.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `transpose_a` and `adjoint_a`, or `transpose_b` and\n",
      " |          `adjoint_b` are both set to `True`.\n",
      " |  \n",
      " |  __mod__ = binary_op_wrapper(x, y)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __mul__ = binary_op_wrapper(x, y)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Compares two variables element-wise for equality.\n",
      " |  \n",
      " |  __neg__ = neg(x, name=None)\n",
      " |      Computes numerical negative value element-wise.\n",
      " |      \n",
      " |      I.e., \\\\(y = -x\\\\).\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __or__ = binary_op_wrapper(x, y)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __pow__ = binary_op_wrapper(x, y)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __radd__ = r_binary_op_wrapper(y, x)\n",
      " |      Dispatches to add for strings and add_v2 for all other types.\n",
      " |  \n",
      " |  __rand__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_and` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rdiv__ = r_binary_op_wrapper(y, x)\n",
      " |      Divide two values using Python 2 semantics.\n",
      " |      \n",
      " |      Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rfloordiv__ = r_binary_op_wrapper(y, x)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.compat.v1.div(x,y)` for integers, but uses\n",
      " |      `tf.floor(tf.compat.v1.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __rmatmul__ = r_binary_op_wrapper(y, x)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication dimensions,\n",
      " |      and any further outer dimensions specify matching batch size.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      A simple 2-D tensor matrix multiplication:\n",
      " |      >>> a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      >>> a  # 2-D tensor\n",
      " |      <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
      " |      array([[1, 2, 3],\n",
      " |             [4, 5, 6]], dtype=int32)>\n",
      " |      >>> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      >>> b  # 2-D tensor\n",
      " |      <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      " |      array([[ 7,  8],\n",
      " |             [ 9, 10],\n",
      " |             [11, 12]], dtype=int32)>\n",
      " |      >>> c = tf.matmul(a, b)\n",
      " |      >>> c  # `a` * `b`\n",
      " |      <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
      " |      array([[ 58,  64],\n",
      " |             [139, 154]], dtype=int32)>\n",
      " |      \n",
      " |      A batch matrix multiplication with batch shape [2]\n",
      " |      >>> a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])\n",
      " |      >>> a  # 3-D tensor\n",
      " |      <tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
      " |      array([[[ 1,  2,  3],\n",
      " |              [ 4,  5,  6]],\n",
      " |             [[ 7,  8,  9],\n",
      " |              [10, 11, 12]]], dtype=int32)>\n",
      " |      >>> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])\n",
      " |      >>> b  # 3-D tensor\n",
      " |      <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
      " |      array([[[13, 14],\n",
      " |              [15, 16],\n",
      " |              [17, 18]],\n",
      " |             [[19, 20],\n",
      " |              [21, 22],\n",
      " |              [23, 24]]], dtype=int32)>\n",
      " |      >>> c = tf.matmul(a, b)\n",
      " |      >>> c  # `a` * `b`\n",
      " |      <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
      " |      array([[[ 94, 100],\n",
      " |              [229, 244]],\n",
      " |             [[508, 532],\n",
      " |              [697, 730]]], dtype=int32)>\n",
      " |      \n",
      " |      Since python >= 3.5 the @ operator is supported\n",
      " |      (see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow,\n",
      " |      it simply calls the `tf.matmul()` function, so the following lines are\n",
      " |      equivalent:\n",
      " |      >>> d = a @ b @ [[10], [11]]\n",
      " |      >>> d = tf.matmul(tf.matmul(a, b), [[10], [11]])\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `tf.Tensor` of type `float16`, `float32`, `float64`, `int32`,\n",
      " |          `complex64`, `complex128` and rank > 1.\n",
      " |        b: `tf.Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of the same type as `a` and `b` where each inner-most matrix\n",
      " |        is the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])`,\n",
      " |        for all indices `i`, `j`.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `transpose_a` and `adjoint_a`, or `transpose_b` and\n",
      " |          `adjoint_b` are both set to `True`.\n",
      " |  \n",
      " |  __rmod__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rmul__ = r_binary_op_wrapper(y, x)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __ror__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rpow__ = r_binary_op_wrapper(y, x)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __rsub__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rtruediv__ = r_binary_op_wrapper(y, x)\n",
      " |  \n",
      " |  __rxor__ = r_binary_op_wrapper(y, x)\n",
      " |      Logical XOR function.\n",
      " |      \n",
      " |      x ^ y = (x | y) & ~(x & y)\n",
      " |      \n",
      " |      Inputs are tensor and if the tensors contains more than one element, an\n",
      " |      element-wise logical XOR is computed.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([False, False, True, True], dtype = tf.bool)\n",
      " |      y = tf.constant([False, True, False, True], dtype = tf.bool)\n",
      " |      z = tf.logical_xor(x, y, name=\"LogicalXor\")\n",
      " |      #  here z = [False  True  True False]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          x: A `Tensor` type bool.\n",
      " |          y: A `Tensor` of type bool.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type bool with the same size as that of x or y.\n",
      " |  \n",
      " |  __sub__ = binary_op_wrapper(x, y)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __truediv__ = binary_op_wrapper(x, y)\n",
      " |  \n",
      " |  __xor__ = binary_op_wrapper(x, y)\n",
      " |      Logical XOR function.\n",
      " |      \n",
      " |      x ^ y = (x | y) & ~(x & y)\n",
      " |      \n",
      " |      Inputs are tensor and if the tensors contains more than one element, an\n",
      " |      element-wise logical XOR is computed.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([False, False, True, True], dtype = tf.bool)\n",
      " |      y = tf.constant([False, True, False, True], dtype = tf.bool)\n",
      " |      z = tf.logical_xor(x, y, name=\"LogicalXor\")\n",
      " |      #  here z = [False  True  True False]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          x: A `Tensor` type bool.\n",
      " |          y: A `Tensor` of type bool.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type bool with the same size as that of x or y.\n",
      " |  \n",
      " |  assign(self, value, use_locking=False, name=None, read_value=True)\n",
      " |      Assigns a new value to the variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `assign(self, value)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: A `Tensor`. The new value for this variable.\n",
      " |        use_locking: If `True`, use locking during the assignment.\n",
      " |        name: The name of the operation to be created\n",
      " |        read_value: if True, will return something which evaluates to the new\n",
      " |          value of the variable; if False will return the assign op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the assignment has completed.\n",
      " |  \n",
      " |  assign_add(self, delta, use_locking=False, name=None, read_value=True)\n",
      " |      Adds a value to this variable.\n",
      " |      \n",
      " |       This is essentially a shortcut for `assign_add(self, delta)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        delta: A `Tensor`. The value to add to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: The name of the operation to be created\n",
      " |        read_value: if True, will return something which evaluates to the new\n",
      " |          value of the variable; if False will return the assign op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the addition has completed.\n",
      " |  \n",
      " |  assign_sub(self, delta, use_locking=False, name=None, read_value=True)\n",
      " |      Subtracts a value from this variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `assign_sub(self, delta)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        delta: A `Tensor`. The value to subtract from this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: The name of the operation to be created\n",
      " |        read_value: if True, will return something which evaluates to the new\n",
      " |          value of the variable; if False will return the assign op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the subtraction has completed.\n",
      " |  \n",
      " |  batch_scatter_update(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Assigns `tf.IndexedSlices` to this variable batch-wise.\n",
      " |      \n",
      " |      Analogous to `batch_gather`. This assumes that this variable and the\n",
      " |      sparse_delta IndexedSlices have a series of leading dimensions that are the\n",
      " |      same for all of them, and the updates are performed on the last dimension of\n",
      " |      indices. In other words, the dimensions should be the following:\n",
      " |      \n",
      " |      `num_prefix_dims = sparse_delta.indices.ndims - 1`\n",
      " |      `batch_dim = num_prefix_dims + 1`\n",
      " |      `sparse_delta.updates.shape = sparse_delta.indices.shape + var.shape[\n",
      " |           batch_dim:]`\n",
      " |      \n",
      " |      where\n",
      " |      \n",
      " |      `sparse_delta.updates.shape[:num_prefix_dims]`\n",
      " |      `== sparse_delta.indices.shape[:num_prefix_dims]`\n",
      " |      `== var.shape[:num_prefix_dims]`\n",
      " |      \n",
      " |      And the operation performed can be expressed as:\n",
      " |      \n",
      " |      `var[i_1, ..., i_n,\n",
      " |           sparse_delta.indices[i_1, ..., i_n, j]] = sparse_delta.updates[\n",
      " |              i_1, ..., i_n, j]`\n",
      " |      \n",
      " |      When sparse_delta.indices is a 1D tensor, this operation is equivalent to\n",
      " |      `scatter_update`.\n",
      " |      \n",
      " |      To avoid this operation one can looping over the first `ndims` of the\n",
      " |      variable and using `scatter_update` on the subtensors that result of slicing\n",
      " |      the first dimension. This is a valid option for `ndims = 1`, but less\n",
      " |      efficient than this implementation.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to be assigned to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered assignment has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  count_up_to(self, limit)\n",
      " |      Increments this variable until it reaches `limit`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Prefer Dataset.range instead.\n",
      " |      \n",
      " |      When that Op is run it tries to increment the variable by `1`. If\n",
      " |      incrementing the variable would bring it above `limit` then the Op raises\n",
      " |      the exception `OutOfRangeError`.\n",
      " |      \n",
      " |      If no error is raised, the Op outputs the value of the variable before\n",
      " |      the increment.\n",
      " |      \n",
      " |      This is essentially a shortcut for `count_up_to(self, limit)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        limit: value at which incrementing the variable raises an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the variable value before the increment. If no\n",
      " |        other Op modifies this variable, the values produced will all be\n",
      " |        distinct.\n",
      " |  \n",
      " |  eval(self, session=None)\n",
      " |      In a session, computes and returns the value of this variable.\n",
      " |      \n",
      " |      This is not a graph construction method, it does not add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph\n",
      " |      containing this variable has been launched. If no session is\n",
      " |      passed, the default session is used.  See `tf.compat.v1.Session` for more\n",
      " |      information on launching a graph and on sessions.\n",
      " |      \n",
      " |      ```python\n",
      " |      v = tf.Variable([1, 2])\n",
      " |      init = tf.compat.v1.global_variables_initializer()\n",
      " |      \n",
      " |      with tf.compat.v1.Session() as sess:\n",
      " |          sess.run(init)\n",
      " |          # Usage passing the session explicitly.\n",
      " |          print(v.eval(sess))\n",
      " |          # Usage with the default session.  The 'with' block\n",
      " |          # above makes 'sess' the default session.\n",
      " |          print(v.eval())\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        session: The session to use to evaluate this variable. If none, the\n",
      " |          default session is used.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A numpy `ndarray` with a copy of the value of this variable.\n",
      " |  \n",
      " |  experimental_ref(self)\n",
      " |      Returns a hashable reference object to this Variable.\n",
      " |      \n",
      " |      Warning: Experimental API that could be changed or removed.\n",
      " |      \n",
      " |      The primary usecase for this API is to put variables in a set/dictionary.\n",
      " |      We can't put variables in a set/dictionary as `variable.__hash__()` is no\n",
      " |      longer available starting Tensorflow 2.0.\n",
      " |      \n",
      " |      ```python\n",
      " |      import tensorflow as tf\n",
      " |      \n",
      " |      x = tf.Variable(5)\n",
      " |      y = tf.Variable(10)\n",
      " |      z = tf.Variable(10)\n",
      " |      \n",
      " |      # The followings will raise an exception starting 2.0\n",
      " |      # TypeError: Variable is unhashable if Variable equality is enabled.\n",
      " |      variable_set = {x, y, z}\n",
      " |      variable_dict = {x: 'five', y: 'ten'}\n",
      " |      ```\n",
      " |      \n",
      " |      Instead, we can use `variable.experimental_ref()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      variable_set = {x.experimental_ref(),\n",
      " |                      y.experimental_ref(),\n",
      " |                      z.experimental_ref()}\n",
      " |      \n",
      " |      print(x.experimental_ref() in variable_set)\n",
      " |      ==> True\n",
      " |      \n",
      " |      variable_dict = {x.experimental_ref(): 'five',\n",
      " |                       y.experimental_ref(): 'ten',\n",
      " |                       z.experimental_ref(): 'ten'}\n",
      " |      \n",
      " |      print(variable_dict[y.experimental_ref()])\n",
      " |      ==> ten\n",
      " |      ```\n",
      " |      \n",
      " |      Also, the reference object provides `.deref()` function that returns the\n",
      " |      original Variable.\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.Variable(5)\n",
      " |      print(x.experimental_ref().deref())\n",
      " |      ==> <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=5>\n",
      " |      ```\n",
      " |  \n",
      " |  gather_nd(self, indices, name=None)\n",
      " |      Gather slices from `params` into a Tensor with shape specified by `indices`.\n",
      " |      \n",
      " |      See tf.gather_nd for details.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      " |          Index tensor.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `params`.\n",
      " |  \n",
      " |  get_shape(self)\n",
      " |      Alias of `Variable.shape`.\n",
      " |  \n",
      " |  initialized_value(self)\n",
      " |      Returns the value of the initialized variable. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      " |      \n",
      " |      You should use this instead of the variable itself to initialize another\n",
      " |      variable with a value that depends on the value of this variable.\n",
      " |      \n",
      " |      ```python\n",
      " |      # Initialize 'v' with a random tensor.\n",
      " |      v = tf.Variable(tf.random.truncated_normal([10, 40]))\n",
      " |      # Use `initialized_value` to guarantee that `v` has been\n",
      " |      # initialized before its value is used to initialize `w`.\n",
      " |      # The random values are picked only once.\n",
      " |      w = tf.Variable(v.initialized_value() * 2.0)\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` holding the value of this variable after its initializer\n",
      " |        has run.\n",
      " |  \n",
      " |  load(self, value, session=None)\n",
      " |      Load new value into this variable. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      " |      \n",
      " |      Writes new value to variable's memory. Doesn't add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph\n",
      " |      containing this variable has been launched. If no session is\n",
      " |      passed, the default session is used.  See `tf.compat.v1.Session` for more\n",
      " |      information on launching a graph and on sessions.\n",
      " |      \n",
      " |      ```python\n",
      " |      v = tf.Variable([1, 2])\n",
      " |      init = tf.compat.v1.global_variables_initializer()\n",
      " |      \n",
      " |      with tf.compat.v1.Session() as sess:\n",
      " |          sess.run(init)\n",
      " |          # Usage passing the session explicitly.\n",
      " |          v.load([2, 3], sess)\n",
      " |          print(v.eval(sess)) # prints [2 3]\n",
      " |          # Usage with the default session.  The 'with' block\n",
      " |          # above makes 'sess' the default session.\n",
      " |          v.load([3, 4], sess)\n",
      " |          print(v.eval()) # prints [3 4]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          value: New variable value\n",
      " |          session: The session to use to evaluate this variable. If none, the\n",
      " |            default session is used.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: Session is not passed and no default session\n",
      " |  \n",
      " |  read_value(self)\n",
      " |      Returns the value of this variable, read in the current context.\n",
      " |      \n",
      " |      Can be different from value() if it's on another device, with control\n",
      " |      dependencies, etc.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` containing the value of the variable.\n",
      " |  \n",
      " |  scatter_add(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Adds `tf.IndexedSlices` to this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to be added to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered addition has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_div(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Divide this variable by `tf.IndexedSlices`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to divide this variable by.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered division has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_max(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Updates this variable with the max of `tf.IndexedSlices` and itself.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to use as an argument of max with this\n",
      " |          variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered maximization has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_min(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Updates this variable with the min of `tf.IndexedSlices` and itself.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to use as an argument of min with this\n",
      " |          variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered minimization has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_mul(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Multiply this variable by `tf.IndexedSlices`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to multiply this variable by.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered multiplication has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_nd_add(self, indices, updates, name=None)\n",
      " |      Applies sparse addition to individual values or slices in a Variable.\n",
      " |      \n",
      " |      The Variable has rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
      " |      \n",
      " |      `indices` must be integer tensor, containing indices into self.\n",
      " |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
      " |      \n",
      " |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
      " |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
      " |      dimension of self.\n",
      " |      \n",
      " |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
      " |      \n",
      " |      ```\n",
      " |      [d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].\n",
      " |      ```\n",
      " |      \n",
      " |      For example, say we want to add 4 scattered elements to a rank-1 tensor to\n",
      " |      8 elements. In Python, that update would look like this:\n",
      " |      \n",
      " |      ```python\n",
      " |          v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n",
      " |          indices = tf.constant([[4], [3], [1] ,[7]])\n",
      " |          updates = tf.constant([9, 10, 11, 12])\n",
      " |          add = v.scatter_nd_add(indices, updates)\n",
      " |          with tf.compat.v1.Session() as sess:\n",
      " |            print sess.run(add)\n",
      " |      ```\n",
      " |      \n",
      " |      The resulting update to v would look like this:\n",
      " |      \n",
      " |          [1, 13, 3, 14, 14, 6, 7, 20]\n",
      " |      \n",
      " |      See `tf.scatter_nd` for more details about how to make updates to\n",
      " |      slices.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: The indices to be used in the operation.\n",
      " |        updates: The values to be used in the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered addition has completed.\n",
      " |  \n",
      " |  scatter_nd_sub(self, indices, updates, name=None)\n",
      " |      Applies sparse subtraction to individual values or slices in a Variable.\n",
      " |      \n",
      " |      Assuming the variable has rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
      " |      \n",
      " |      `indices` must be integer tensor, containing indices into self.\n",
      " |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
      " |      \n",
      " |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
      " |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
      " |      dimension of self.\n",
      " |      \n",
      " |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
      " |      \n",
      " |      ```\n",
      " |      [d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].\n",
      " |      ```\n",
      " |      \n",
      " |      For example, say we want to add 4 scattered elements to a rank-1 tensor to\n",
      " |      8 elements. In Python, that update would look like this:\n",
      " |      \n",
      " |      ```python\n",
      " |          v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n",
      " |          indices = tf.constant([[4], [3], [1] ,[7]])\n",
      " |          updates = tf.constant([9, 10, 11, 12])\n",
      " |          op = v.scatter_nd_sub(indices, updates)\n",
      " |          with tf.compat.v1.Session() as sess:\n",
      " |            print sess.run(op)\n",
      " |      ```\n",
      " |      \n",
      " |      The resulting update to v would look like this:\n",
      " |      \n",
      " |          [1, -9, 3, -6, -6, 6, 7, -4]\n",
      " |      \n",
      " |      See `tf.scatter_nd` for more details about how to make updates to\n",
      " |      slices.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: The indices to be used in the operation.\n",
      " |        updates: The values to be used in the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered subtraction has completed.\n",
      " |  \n",
      " |  scatter_nd_update(self, indices, updates, name=None)\n",
      " |      Applies sparse assignment to individual values or slices in a Variable.\n",
      " |      \n",
      " |      The Variable has rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
      " |      \n",
      " |      `indices` must be integer tensor, containing indices into self.\n",
      " |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
      " |      \n",
      " |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
      " |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
      " |      dimension of self.\n",
      " |      \n",
      " |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
      " |      \n",
      " |      ```\n",
      " |      [d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].\n",
      " |      ```\n",
      " |      \n",
      " |      For example, say we want to add 4 scattered elements to a rank-1 tensor to\n",
      " |      8 elements. In Python, that update would look like this:\n",
      " |      \n",
      " |      ```python\n",
      " |          v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n",
      " |          indices = tf.constant([[4], [3], [1] ,[7]])\n",
      " |          updates = tf.constant([9, 10, 11, 12])\n",
      " |          op = v.scatter_nd_assign(indices, updates)\n",
      " |          with tf.compat.v1.Session() as sess:\n",
      " |            print sess.run(op)\n",
      " |      ```\n",
      " |      \n",
      " |      The resulting update to v would look like this:\n",
      " |      \n",
      " |          [1, 11, 3, 10, 9, 6, 7, 12]\n",
      " |      \n",
      " |      See `tf.scatter_nd` for more details about how to make updates to\n",
      " |      slices.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: The indices to be used in the operation.\n",
      " |        updates: The values to be used in the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered assignment has completed.\n",
      " |  \n",
      " |  scatter_sub(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Subtracts `tf.IndexedSlices` from this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to be subtracted from this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered subtraction has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_update(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Assigns `tf.IndexedSlices` to this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to be assigned to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered assignment has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  set_shape(self, shape)\n",
      " |      Overrides the shape for this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        shape: the `TensorShape` representing the overridden shape.\n",
      " |  \n",
      " |  sparse_read(self, indices, name=None)\n",
      " |      Gather slices from params axis axis according to indices.\n",
      " |      \n",
      " |      This function supports a subset of tf.gather, see tf.gather for details on\n",
      " |      usage.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: The index `Tensor`.  Must be one of the following types: `int32`,\n",
      " |          `int64`. Must be in range `[0, params.shape[axis])`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `params`.\n",
      " |  \n",
      " |  to_proto(self, export_scope=None)\n",
      " |      Converts a `Variable` to a `VariableDef` protocol buffer.\n",
      " |      \n",
      " |      Args:\n",
      " |        export_scope: Optional `string`. Name scope to remove.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `VariableDef` protocol buffer, or `None` if the `Variable` is not\n",
      " |        in the specified name scope.\n",
      " |  \n",
      " |  value(self)\n",
      " |      Returns the last snapshot of this variable.\n",
      " |      \n",
      " |      You usually do not need to call this method as all ops that need the value\n",
      " |      of the variable call it automatically through a `convert_to_tensor()` call.\n",
      " |      \n",
      " |      Returns a `Tensor` which holds the value of the variable.  You can not\n",
      " |      assign a new value to this tensor as it is not a reference to the variable.\n",
      " |      \n",
      " |      To avoid copies, if the consumer of the returned value is on the same device\n",
      " |      as the variable, this actually returns the live value of the variable, not\n",
      " |      a copy.  Updates to the variable are seen by the consumer.  If the consumer\n",
      " |      is on a different device it will get a copy of the variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` containing the value of the variable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_proto(variable_def, import_scope=None)\n",
      " |      Returns a `Variable` object created from `variable_def`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  aggregation\n",
      " |  \n",
      " |  constraint\n",
      " |      Returns the constraint function associated with this variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The constraint function that was passed to the variable constructor.\n",
      " |        Can be `None` if no constraint was passed.\n",
      " |  \n",
      " |  device\n",
      " |      The device of this variable.\n",
      " |  \n",
      " |  dtype\n",
      " |      The `DType` of this variable.\n",
      " |  \n",
      " |  graph\n",
      " |      The `Graph` of this variable.\n",
      " |  \n",
      " |  initial_value\n",
      " |      Returns the Tensor used as the initial value for the variable.\n",
      " |      \n",
      " |      Note that this is different from `initialized_value()` which runs\n",
      " |      the op that initializes the variable before returning its value.\n",
      " |      This method returns the tensor that is used by the op that initializes\n",
      " |      the variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  initializer\n",
      " |      The initializer operation for this variable.\n",
      " |  \n",
      " |  name\n",
      " |      The name of this variable.\n",
      " |  \n",
      " |  op\n",
      " |      The `Operation` of this variable.\n",
      " |  \n",
      " |  shape\n",
      " |      The `TensorShape` of this variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `TensorShape`.\n",
      " |  \n",
      " |  synchronization\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  SaveSliceInfo = <class 'tensorflow.python.ops.variables.Variable.SaveS...\n",
      " |      Information on how to save this Variable as a slice.\n",
      " |      \n",
      " |      Provides internal support for saving variables as slices of a larger\n",
      " |      variable.  This API is not public and is subject to change.\n",
      " |      \n",
      " |      Available properties:\n",
      " |      \n",
      " |      * full_name\n",
      " |      * full_shape\n",
      " |      * var_offset\n",
      " |      * var_shape\n",
      " |  \n",
      " |  __array_priority__ = 100\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.Variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변수를 생성하면 하나의 변수 객체가 만들어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(0,name=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'a:0' shape=() dtype=int32, numpy=0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변수값을 assign 메소드로 변경이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int32, numpy=1>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.assign(a+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변수의 값을 보면 실제 tensor로 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=1>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=1>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.read_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
